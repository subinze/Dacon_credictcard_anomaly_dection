{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VUSDWk_4JB-3"},"outputs":[],"source":["cnfimport pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","\n","import matplotlib.pyplot as plt\n","\n","import random\n","import os\n","\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KE06sYaxJB_M"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IX_WN-0hJqhx","executionInfo":{"status":"ok","timestamp":1658751667612,"user_tz":-540,"elapsed":18471,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"c39cfb8b-9164-40b5-dd78-7a068113b766"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"yZQ2vzJYJB_P"},"source":["# 데이터 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"HpfjiAqYJB_V","executionInfo":{"status":"ok","timestamp":1658751669392,"user_tz":-540,"elapsed":1806,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"5d3c2eee-cfce-41a0-ba8f-de2eb9ed90fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ID        V1        V2        V3        V4        V5        V6        V7  \\\n","0   3 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","1   4 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","2   6 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n","3   8 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n","4   9 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n","\n","         V8        V9  ...       V21       V22       V23       V24       V25  \\\n","0  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n","1  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n","2  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n","3 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n","4  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n","\n","        V26       V27       V28       V29       V30  \n","0 -0.139097 -0.055353 -0.059752  4.983721 -0.994972  \n","1 -0.221929  0.062723  0.061458  1.418291 -0.994972  \n","2  0.105915  0.253844  0.081080 -0.256131 -0.994960  \n","3 -0.051634 -1.206921 -1.085339  0.262698 -0.994901  \n","4 -0.384157  0.011747  0.142404  0.994900 -0.994901  \n","\n","[5 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-9d87b79a-1f62-4428-9b0e-1559983a6b55\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>4.983721</td>\n","      <td>-0.994972</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>1.418291</td>\n","      <td>-0.994972</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>-0.425966</td>\n","      <td>0.960523</td>\n","      <td>1.141109</td>\n","      <td>-0.168252</td>\n","      <td>0.420987</td>\n","      <td>-0.029728</td>\n","      <td>0.476201</td>\n","      <td>0.260314</td>\n","      <td>-0.568671</td>\n","      <td>...</td>\n","      <td>-0.208254</td>\n","      <td>-0.559825</td>\n","      <td>-0.026398</td>\n","      <td>-0.371427</td>\n","      <td>-0.232794</td>\n","      <td>0.105915</td>\n","      <td>0.253844</td>\n","      <td>0.081080</td>\n","      <td>-0.256131</td>\n","      <td>-0.994960</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8</td>\n","      <td>-0.644269</td>\n","      <td>1.417964</td>\n","      <td>1.074380</td>\n","      <td>-0.492199</td>\n","      <td>0.948934</td>\n","      <td>0.428118</td>\n","      <td>1.120631</td>\n","      <td>-3.807864</td>\n","      <td>0.615375</td>\n","      <td>...</td>\n","      <td>1.943465</td>\n","      <td>-1.015455</td>\n","      <td>0.057504</td>\n","      <td>-0.649709</td>\n","      <td>-0.415267</td>\n","      <td>-0.051634</td>\n","      <td>-1.206921</td>\n","      <td>-1.085339</td>\n","      <td>0.262698</td>\n","      <td>-0.994901</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>-0.894286</td>\n","      <td>0.286157</td>\n","      <td>-0.113192</td>\n","      <td>-0.271526</td>\n","      <td>2.669599</td>\n","      <td>3.721818</td>\n","      <td>0.370145</td>\n","      <td>0.851084</td>\n","      <td>-0.392048</td>\n","      <td>...</td>\n","      <td>-0.073425</td>\n","      <td>-0.268092</td>\n","      <td>-0.204233</td>\n","      <td>1.011592</td>\n","      <td>0.373205</td>\n","      <td>-0.384157</td>\n","      <td>0.011747</td>\n","      <td>0.142404</td>\n","      <td>0.994900</td>\n","      <td>-0.994901</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d87b79a-1f62-4428-9b0e-1559983a6b55')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9d87b79a-1f62-4428-9b0e-1559983a6b55 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9d87b79a-1f62-4428-9b0e-1559983a6b55');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["train_df = pd.read_csv('/content/drive/MyDrive/[데이콘] 신용카드 거래/train.csv') # Train\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"yieHiiofJB_Z","executionInfo":{"status":"ok","timestamp":1658751670538,"user_tz":-540,"elapsed":1155,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"96e48ced-c4bd-43de-f09c-47a1a2c3bb2e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ID        V1        V2        V3        V4        V5        V6        V7  \\\n","0  10 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n","1  22  0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038  0.107712   \n","2  63  1.145524  0.575068  0.194008  2.598192 -0.092210 -1.044430  0.531588   \n","3  69  0.927060 -0.323684  0.387585  0.544474  0.246787  1.650358 -0.427576   \n","4  83 -3.005237  2.600138  1.483691 -2.418473  0.306326 -0.824575  2.065426   \n","\n","         V8        V9  ...       V22       V23       V24       V25       V26  \\\n","0  0.069539 -0.736727  ... -0.633753 -0.120794 -0.385050 -0.069733  0.094199   \n","1  0.521502 -1.191311  ...  0.402492 -0.048508 -1.371866  0.390814  0.199964   \n","2 -0.241888 -0.896287  ... -0.119703 -0.076510  0.691320  0.633984  0.048741   \n","3  0.615371  0.226278  ...  0.079359  0.096632 -0.992569  0.085096  0.377447   \n","4 -1.829347  4.009259  ... -0.181268 -0.163747  0.515821  0.136318  0.460054   \n","\n","        V27       V28       V29       V30  Class  \n","0  0.246219  0.083076 -0.255991 -0.994878      0  \n","1  0.016371 -0.014605  0.168937 -0.994784      0  \n","2 -0.053192  0.016251  0.169496 -0.994502      0  \n","3  0.036096 -0.005960  0.331307 -0.994467      0  \n","4 -0.251259 -1.105751 -0.287012 -0.994373      0  \n","\n","[5 rows x 32 columns]"],"text/html":["\n","  <div id=\"df-a8218e8a-4459-40e3-8dc0-27326d3d1312\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10</td>\n","      <td>-0.338262</td>\n","      <td>1.119593</td>\n","      <td>1.044367</td>\n","      <td>-0.222187</td>\n","      <td>0.499361</td>\n","      <td>-0.246761</td>\n","      <td>0.651583</td>\n","      <td>0.069539</td>\n","      <td>-0.736727</td>\n","      <td>...</td>\n","      <td>-0.633753</td>\n","      <td>-0.120794</td>\n","      <td>-0.385050</td>\n","      <td>-0.069733</td>\n","      <td>0.094199</td>\n","      <td>0.246219</td>\n","      <td>0.083076</td>\n","      <td>-0.255991</td>\n","      <td>-0.994878</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22</td>\n","      <td>0.962496</td>\n","      <td>0.328461</td>\n","      <td>-0.171479</td>\n","      <td>2.109204</td>\n","      <td>1.129566</td>\n","      <td>1.696038</td>\n","      <td>0.107712</td>\n","      <td>0.521502</td>\n","      <td>-1.191311</td>\n","      <td>...</td>\n","      <td>0.402492</td>\n","      <td>-0.048508</td>\n","      <td>-1.371866</td>\n","      <td>0.390814</td>\n","      <td>0.199964</td>\n","      <td>0.016371</td>\n","      <td>-0.014605</td>\n","      <td>0.168937</td>\n","      <td>-0.994784</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>63</td>\n","      <td>1.145524</td>\n","      <td>0.575068</td>\n","      <td>0.194008</td>\n","      <td>2.598192</td>\n","      <td>-0.092210</td>\n","      <td>-1.044430</td>\n","      <td>0.531588</td>\n","      <td>-0.241888</td>\n","      <td>-0.896287</td>\n","      <td>...</td>\n","      <td>-0.119703</td>\n","      <td>-0.076510</td>\n","      <td>0.691320</td>\n","      <td>0.633984</td>\n","      <td>0.048741</td>\n","      <td>-0.053192</td>\n","      <td>0.016251</td>\n","      <td>0.169496</td>\n","      <td>-0.994502</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>69</td>\n","      <td>0.927060</td>\n","      <td>-0.323684</td>\n","      <td>0.387585</td>\n","      <td>0.544474</td>\n","      <td>0.246787</td>\n","      <td>1.650358</td>\n","      <td>-0.427576</td>\n","      <td>0.615371</td>\n","      <td>0.226278</td>\n","      <td>...</td>\n","      <td>0.079359</td>\n","      <td>0.096632</td>\n","      <td>-0.992569</td>\n","      <td>0.085096</td>\n","      <td>0.377447</td>\n","      <td>0.036096</td>\n","      <td>-0.005960</td>\n","      <td>0.331307</td>\n","      <td>-0.994467</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>83</td>\n","      <td>-3.005237</td>\n","      <td>2.600138</td>\n","      <td>1.483691</td>\n","      <td>-2.418473</td>\n","      <td>0.306326</td>\n","      <td>-0.824575</td>\n","      <td>2.065426</td>\n","      <td>-1.829347</td>\n","      <td>4.009259</td>\n","      <td>...</td>\n","      <td>-0.181268</td>\n","      <td>-0.163747</td>\n","      <td>0.515821</td>\n","      <td>0.136318</td>\n","      <td>0.460054</td>\n","      <td>-0.251259</td>\n","      <td>-1.105751</td>\n","      <td>-0.287012</td>\n","      <td>-0.994373</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 32 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8218e8a-4459-40e3-8dc0-27326d3d1312')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a8218e8a-4459-40e3-8dc0-27326d3d1312 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a8218e8a-4459-40e3-8dc0-27326d3d1312');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["val_df = pd.read_csv('/content/drive/MyDrive/[데이콘] 신용카드 거래/val.csv') # Validation\n","val_df.head()"]},{"cell_type":"markdown","metadata":{"id":"BFPGv083JB_a"},"source":["## 데이터 개수 확인\n","- 생각보다 min, max값이 피쳐마다 많이 차이나는 것 확인 -> Nomalization 한 번 해보는 것도 괜찮을 듯,,,"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4leYvywrJB_c","executionInfo":{"status":"ok","timestamp":1658751670540,"user_tz":-540,"elapsed":30,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"941a5936-5db6-4263-c649-af6d221aa777"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(113842, 31)"]},"metadata":{},"execution_count":6}],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"59UFW3DSJB_f","executionInfo":{"status":"ok","timestamp":1658751670540,"user_tz":-540,"elapsed":27,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"2204e78a-8e45-4169-eea8-af8f5499d2bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  ID             V1             V2             V3  \\\n","count  113842.000000  113842.000000  113842.000000  113842.000000   \n","mean   142025.527837       0.000197       0.001289       0.009717   \n","std     82248.545392       1.951060       1.651064       1.496916   \n","min         3.000000     -56.407510     -72.715728     -32.454198   \n","25%     70796.750000      -0.923479      -0.595602      -0.883877   \n","50%    141722.000000       0.012074       0.066390       0.183868   \n","75%    213359.500000       1.315373       0.801687       1.037120   \n","max    284803.000000       2.454930      21.467203       4.187811   \n","\n","                  V4             V5             V6             V7  \\\n","count  113842.000000  113842.000000  113842.000000  113842.000000   \n","mean       -0.004169       0.000475       0.005141       0.005769   \n","std         1.412633       1.367533       1.330583       1.204111   \n","min        -5.600607     -42.147898     -26.160506     -41.506796   \n","25%        -0.853728      -0.689853      -0.766094      -0.552071   \n","50%        -0.019359      -0.054060      -0.272436       0.039036   \n","75%         0.742208       0.614214       0.405285       0.568750   \n","max        16.491217      34.801666      23.917837      44.054461   \n","\n","                  V8             V9  ...            V21           V22  \\\n","count  113842.000000  113842.000000  ...  113842.000000  1.138420e+05   \n","mean       -0.002451      -0.002107  ...      -0.001242  4.088347e-07   \n","std         1.185504       1.095415  ...       0.722001  7.238291e-01   \n","min       -50.943369     -13.434066  ...     -22.757540 -8.887017e+00   \n","25%        -0.209492      -0.647477  ...      -0.229710 -5.402665e-01   \n","50%         0.020970      -0.052157  ...      -0.030281  8.345807e-03   \n","75%         0.328303       0.590705  ...       0.186001  5.287508e-01   \n","max        20.007208      10.392889  ...      27.202839  8.361985e+00   \n","\n","                 V23            V24            V25            V26  \\\n","count  113842.000000  113842.000000  113842.000000  113842.000000   \n","mean       -0.001317      -0.000884       0.001680      -0.000293   \n","std         0.636061       0.605854       0.520069       0.480979   \n","min       -44.807735      -2.824849     -10.295397      -1.855355   \n","25%        -0.162180      -0.355582      -0.315470      -0.326160   \n","50%        -0.012261       0.040573       0.018278      -0.052815   \n","75%         0.147474       0.438225       0.353989       0.240838   \n","max        22.528412       4.022866       7.519589       3.119295   \n","\n","                 V27            V28            V29            V30  \n","count  113842.000000  113842.000000  113842.000000  113842.000000  \n","mean       -0.000234      -0.000508       0.927253       0.116232  \n","std         0.399505       0.356130       3.412933       0.558161  \n","min        -9.895244      -9.617915      -0.307413      -0.994972  \n","25%        -0.070847      -0.053249      -0.230560      -0.360304  \n","50%         0.001502       0.011158      -0.000699      -0.002590  \n","75%         0.091279       0.077851       0.768532       0.640653  \n","max        11.135740      33.847808     180.101027       1.034951  \n","\n","[8 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-35e6f124-c9b5-4996-a2c8-1138a8c68cce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>...</td>\n","      <td>113842.000000</td>\n","      <td>1.138420e+05</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","      <td>113842.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>142025.527837</td>\n","      <td>0.000197</td>\n","      <td>0.001289</td>\n","      <td>0.009717</td>\n","      <td>-0.004169</td>\n","      <td>0.000475</td>\n","      <td>0.005141</td>\n","      <td>0.005769</td>\n","      <td>-0.002451</td>\n","      <td>-0.002107</td>\n","      <td>...</td>\n","      <td>-0.001242</td>\n","      <td>4.088347e-07</td>\n","      <td>-0.001317</td>\n","      <td>-0.000884</td>\n","      <td>0.001680</td>\n","      <td>-0.000293</td>\n","      <td>-0.000234</td>\n","      <td>-0.000508</td>\n","      <td>0.927253</td>\n","      <td>0.116232</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>82248.545392</td>\n","      <td>1.951060</td>\n","      <td>1.651064</td>\n","      <td>1.496916</td>\n","      <td>1.412633</td>\n","      <td>1.367533</td>\n","      <td>1.330583</td>\n","      <td>1.204111</td>\n","      <td>1.185504</td>\n","      <td>1.095415</td>\n","      <td>...</td>\n","      <td>0.722001</td>\n","      <td>7.238291e-01</td>\n","      <td>0.636061</td>\n","      <td>0.605854</td>\n","      <td>0.520069</td>\n","      <td>0.480979</td>\n","      <td>0.399505</td>\n","      <td>0.356130</td>\n","      <td>3.412933</td>\n","      <td>0.558161</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>3.000000</td>\n","      <td>-56.407510</td>\n","      <td>-72.715728</td>\n","      <td>-32.454198</td>\n","      <td>-5.600607</td>\n","      <td>-42.147898</td>\n","      <td>-26.160506</td>\n","      <td>-41.506796</td>\n","      <td>-50.943369</td>\n","      <td>-13.434066</td>\n","      <td>...</td>\n","      <td>-22.757540</td>\n","      <td>-8.887017e+00</td>\n","      <td>-44.807735</td>\n","      <td>-2.824849</td>\n","      <td>-10.295397</td>\n","      <td>-1.855355</td>\n","      <td>-9.895244</td>\n","      <td>-9.617915</td>\n","      <td>-0.307413</td>\n","      <td>-0.994972</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>70796.750000</td>\n","      <td>-0.923479</td>\n","      <td>-0.595602</td>\n","      <td>-0.883877</td>\n","      <td>-0.853728</td>\n","      <td>-0.689853</td>\n","      <td>-0.766094</td>\n","      <td>-0.552071</td>\n","      <td>-0.209492</td>\n","      <td>-0.647477</td>\n","      <td>...</td>\n","      <td>-0.229710</td>\n","      <td>-5.402665e-01</td>\n","      <td>-0.162180</td>\n","      <td>-0.355582</td>\n","      <td>-0.315470</td>\n","      <td>-0.326160</td>\n","      <td>-0.070847</td>\n","      <td>-0.053249</td>\n","      <td>-0.230560</td>\n","      <td>-0.360304</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>141722.000000</td>\n","      <td>0.012074</td>\n","      <td>0.066390</td>\n","      <td>0.183868</td>\n","      <td>-0.019359</td>\n","      <td>-0.054060</td>\n","      <td>-0.272436</td>\n","      <td>0.039036</td>\n","      <td>0.020970</td>\n","      <td>-0.052157</td>\n","      <td>...</td>\n","      <td>-0.030281</td>\n","      <td>8.345807e-03</td>\n","      <td>-0.012261</td>\n","      <td>0.040573</td>\n","      <td>0.018278</td>\n","      <td>-0.052815</td>\n","      <td>0.001502</td>\n","      <td>0.011158</td>\n","      <td>-0.000699</td>\n","      <td>-0.002590</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>213359.500000</td>\n","      <td>1.315373</td>\n","      <td>0.801687</td>\n","      <td>1.037120</td>\n","      <td>0.742208</td>\n","      <td>0.614214</td>\n","      <td>0.405285</td>\n","      <td>0.568750</td>\n","      <td>0.328303</td>\n","      <td>0.590705</td>\n","      <td>...</td>\n","      <td>0.186001</td>\n","      <td>5.287508e-01</td>\n","      <td>0.147474</td>\n","      <td>0.438225</td>\n","      <td>0.353989</td>\n","      <td>0.240838</td>\n","      <td>0.091279</td>\n","      <td>0.077851</td>\n","      <td>0.768532</td>\n","      <td>0.640653</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>284803.000000</td>\n","      <td>2.454930</td>\n","      <td>21.467203</td>\n","      <td>4.187811</td>\n","      <td>16.491217</td>\n","      <td>34.801666</td>\n","      <td>23.917837</td>\n","      <td>44.054461</td>\n","      <td>20.007208</td>\n","      <td>10.392889</td>\n","      <td>...</td>\n","      <td>27.202839</td>\n","      <td>8.361985e+00</td>\n","      <td>22.528412</td>\n","      <td>4.022866</td>\n","      <td>7.519589</td>\n","      <td>3.119295</td>\n","      <td>11.135740</td>\n","      <td>33.847808</td>\n","      <td>180.101027</td>\n","      <td>1.034951</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 31 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35e6f124-c9b5-4996-a2c8-1138a8c68cce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35e6f124-c9b5-4996-a2c8-1138a8c68cce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35e6f124-c9b5-4996-a2c8-1138a8c68cce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["train_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2KmHKp1JB_i","executionInfo":{"status":"ok","timestamp":1658751670541,"user_tz":-540,"elapsed":25,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"40d2912b-0efe-4fbc-b5b6-36754aa25df0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28462, 32)"]},"metadata":{},"execution_count":8}],"source":["val_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"aRpbuZ7mJB_j","executionInfo":{"status":"ok","timestamp":1658751670542,"user_tz":-540,"elapsed":21,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"9f82a2a3-0621-411a-8074-4f38b00ef953"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  ID            V1            V2            V3            V4  \\\n","count   28462.000000  28462.000000  28462.000000  28462.000000  28462.000000   \n","mean   142549.840138      0.004967      0.002014      0.001414      0.001893   \n","std     82252.851474      1.930640      1.605198      1.499749      1.405139   \n","min        10.000000    -29.516123    -38.305310    -30.177317     -5.071241   \n","25%     71260.000000     -0.915525     -0.598053     -0.873022     -0.852444   \n","50%    142683.500000      0.023586      0.075470      0.175784     -0.021619   \n","75%    213612.750000      1.315578      0.803463      1.011563      0.739044   \n","max    284806.000000      2.411769     16.497472      4.226108     12.114672   \n","\n","                 V5            V6            V7            V8            V9  \\\n","count  28462.000000  28462.000000  28462.000000  28462.000000  28462.000000   \n","mean      -0.003969     -0.017730      0.005556      0.009023     -0.004905   \n","std        1.335147      1.292214      1.165132      1.103538      1.090843   \n","min      -21.577019    -16.172614    -31.197329    -26.278007     -9.462573   \n","25%       -0.698010     -0.774920     -0.546290     -0.210941     -0.642760   \n","50%       -0.052780     -0.280742      0.046280      0.022622     -0.066623   \n","75%        0.598712      0.377266      0.566825      0.323836      0.596308   \n","max       24.345310     12.128950     26.237722     12.431140      7.937413   \n","\n","       ...           V22           V23           V24           V25  \\\n","count  ...  28462.000000  28462.000000  28462.000000  28462.000000   \n","mean   ...      0.001224      0.001490      0.001355      0.002284   \n","std    ...      0.721350      0.588076      0.603181      0.526864   \n","min    ...     -8.555808    -25.356744     -2.807897     -6.035054   \n","25%    ...     -0.542624     -0.160418     -0.352310     -0.322098   \n","50%    ...      0.008401     -0.011372      0.040076      0.016684   \n","75%    ...      0.528163      0.146094      0.441402      0.352930   \n","max    ...      6.090514     18.946734      3.658746      5.525093   \n","\n","                V26           V27           V28           V29           V30  \\\n","count  28462.000000  28462.000000  28462.000000  28462.000000  28462.000000   \n","mean       0.000520     -0.000520     -0.000265      0.924010      0.119916   \n","std        0.484804      0.394235      0.304284      3.347555      0.558246   \n","min       -1.596493     -9.793568     -8.364853     -0.307413     -0.994878   \n","25%       -0.328638     -0.071787     -0.052361     -0.226927     -0.357884   \n","50%       -0.049583      0.000488      0.012138      0.005589      0.002156   \n","75%        0.241051      0.089100      0.080822      0.778278      0.641828   \n","max        3.067907      8.708972     15.726807    165.948299      1.034975   \n","\n","              Class  \n","count  28462.000000  \n","mean       0.001054  \n","std        0.032449  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        1.000000  \n","\n","[8 rows x 32 columns]"],"text/html":["\n","  <div id=\"df-4a05b79c-ce6e-4b8f-b35b-f423a62a53b1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>...</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","      <td>28462.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>142549.840138</td>\n","      <td>0.004967</td>\n","      <td>0.002014</td>\n","      <td>0.001414</td>\n","      <td>0.001893</td>\n","      <td>-0.003969</td>\n","      <td>-0.017730</td>\n","      <td>0.005556</td>\n","      <td>0.009023</td>\n","      <td>-0.004905</td>\n","      <td>...</td>\n","      <td>0.001224</td>\n","      <td>0.001490</td>\n","      <td>0.001355</td>\n","      <td>0.002284</td>\n","      <td>0.000520</td>\n","      <td>-0.000520</td>\n","      <td>-0.000265</td>\n","      <td>0.924010</td>\n","      <td>0.119916</td>\n","      <td>0.001054</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>82252.851474</td>\n","      <td>1.930640</td>\n","      <td>1.605198</td>\n","      <td>1.499749</td>\n","      <td>1.405139</td>\n","      <td>1.335147</td>\n","      <td>1.292214</td>\n","      <td>1.165132</td>\n","      <td>1.103538</td>\n","      <td>1.090843</td>\n","      <td>...</td>\n","      <td>0.721350</td>\n","      <td>0.588076</td>\n","      <td>0.603181</td>\n","      <td>0.526864</td>\n","      <td>0.484804</td>\n","      <td>0.394235</td>\n","      <td>0.304284</td>\n","      <td>3.347555</td>\n","      <td>0.558246</td>\n","      <td>0.032449</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>10.000000</td>\n","      <td>-29.516123</td>\n","      <td>-38.305310</td>\n","      <td>-30.177317</td>\n","      <td>-5.071241</td>\n","      <td>-21.577019</td>\n","      <td>-16.172614</td>\n","      <td>-31.197329</td>\n","      <td>-26.278007</td>\n","      <td>-9.462573</td>\n","      <td>...</td>\n","      <td>-8.555808</td>\n","      <td>-25.356744</td>\n","      <td>-2.807897</td>\n","      <td>-6.035054</td>\n","      <td>-1.596493</td>\n","      <td>-9.793568</td>\n","      <td>-8.364853</td>\n","      <td>-0.307413</td>\n","      <td>-0.994878</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>71260.000000</td>\n","      <td>-0.915525</td>\n","      <td>-0.598053</td>\n","      <td>-0.873022</td>\n","      <td>-0.852444</td>\n","      <td>-0.698010</td>\n","      <td>-0.774920</td>\n","      <td>-0.546290</td>\n","      <td>-0.210941</td>\n","      <td>-0.642760</td>\n","      <td>...</td>\n","      <td>-0.542624</td>\n","      <td>-0.160418</td>\n","      <td>-0.352310</td>\n","      <td>-0.322098</td>\n","      <td>-0.328638</td>\n","      <td>-0.071787</td>\n","      <td>-0.052361</td>\n","      <td>-0.226927</td>\n","      <td>-0.357884</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>142683.500000</td>\n","      <td>0.023586</td>\n","      <td>0.075470</td>\n","      <td>0.175784</td>\n","      <td>-0.021619</td>\n","      <td>-0.052780</td>\n","      <td>-0.280742</td>\n","      <td>0.046280</td>\n","      <td>0.022622</td>\n","      <td>-0.066623</td>\n","      <td>...</td>\n","      <td>0.008401</td>\n","      <td>-0.011372</td>\n","      <td>0.040076</td>\n","      <td>0.016684</td>\n","      <td>-0.049583</td>\n","      <td>0.000488</td>\n","      <td>0.012138</td>\n","      <td>0.005589</td>\n","      <td>0.002156</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>213612.750000</td>\n","      <td>1.315578</td>\n","      <td>0.803463</td>\n","      <td>1.011563</td>\n","      <td>0.739044</td>\n","      <td>0.598712</td>\n","      <td>0.377266</td>\n","      <td>0.566825</td>\n","      <td>0.323836</td>\n","      <td>0.596308</td>\n","      <td>...</td>\n","      <td>0.528163</td>\n","      <td>0.146094</td>\n","      <td>0.441402</td>\n","      <td>0.352930</td>\n","      <td>0.241051</td>\n","      <td>0.089100</td>\n","      <td>0.080822</td>\n","      <td>0.778278</td>\n","      <td>0.641828</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>284806.000000</td>\n","      <td>2.411769</td>\n","      <td>16.497472</td>\n","      <td>4.226108</td>\n","      <td>12.114672</td>\n","      <td>24.345310</td>\n","      <td>12.128950</td>\n","      <td>26.237722</td>\n","      <td>12.431140</td>\n","      <td>7.937413</td>\n","      <td>...</td>\n","      <td>6.090514</td>\n","      <td>18.946734</td>\n","      <td>3.658746</td>\n","      <td>5.525093</td>\n","      <td>3.067907</td>\n","      <td>8.708972</td>\n","      <td>15.726807</td>\n","      <td>165.948299</td>\n","      <td>1.034975</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 32 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a05b79c-ce6e-4b8f-b35b-f423a62a53b1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4a05b79c-ce6e-4b8f-b35b-f423a62a53b1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4a05b79c-ce6e-4b8f-b35b-f423a62a53b1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["val_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"IHxKZpiUJB_k"},"source":["## 데이터 칼럼 변경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WF7hxzgwJB_l"},"outputs":[],"source":["train_df = train_df.drop('ID', axis = 1)\n","val_df = val_df.drop('ID', axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"ysHVsUtYJB_m","executionInfo":{"status":"ok","timestamp":1658751670543,"user_tz":-540,"elapsed":20,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"787d4ae3-cd32-4c8e-f86c-9b02b08185f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         V1        V2        V3        V4        V5        V6        V7  \\\n","0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","2 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n","3 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n","4 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n","\n","         V8        V9       V10  ...       V21       V22       V23       V24  \\\n","0  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n","1  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n","2  0.260314 -0.568671 -0.371407  ... -0.208254 -0.559825 -0.026398 -0.371427   \n","3 -3.807864  0.615375  1.249376  ...  1.943465 -1.015455  0.057504 -0.649709   \n","4  0.851084 -0.392048 -0.410430  ... -0.073425 -0.268092 -0.204233  1.011592   \n","\n","        V25       V26       V27       V28       V29       V30  \n","0 -0.327642 -0.139097 -0.055353 -0.059752  4.983721 -0.994972  \n","1  0.647376 -0.221929  0.062723  0.061458  1.418291 -0.994972  \n","2 -0.232794  0.105915  0.253844  0.081080 -0.256131 -0.994960  \n","3 -0.415267 -0.051634 -1.206921 -1.085339  0.262698 -0.994901  \n","4  0.373205 -0.384157  0.011747  0.142404  0.994900 -0.994901  \n","\n","[5 rows x 30 columns]"],"text/html":["\n","  <div id=\"df-45a9a5af-9d28-4755-8143-aea22006849d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>V10</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>0.207643</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>4.983721</td>\n","      <td>-0.994972</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>-0.054952</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>1.418291</td>\n","      <td>-0.994972</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.425966</td>\n","      <td>0.960523</td>\n","      <td>1.141109</td>\n","      <td>-0.168252</td>\n","      <td>0.420987</td>\n","      <td>-0.029728</td>\n","      <td>0.476201</td>\n","      <td>0.260314</td>\n","      <td>-0.568671</td>\n","      <td>-0.371407</td>\n","      <td>...</td>\n","      <td>-0.208254</td>\n","      <td>-0.559825</td>\n","      <td>-0.026398</td>\n","      <td>-0.371427</td>\n","      <td>-0.232794</td>\n","      <td>0.105915</td>\n","      <td>0.253844</td>\n","      <td>0.081080</td>\n","      <td>-0.256131</td>\n","      <td>-0.994960</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.644269</td>\n","      <td>1.417964</td>\n","      <td>1.074380</td>\n","      <td>-0.492199</td>\n","      <td>0.948934</td>\n","      <td>0.428118</td>\n","      <td>1.120631</td>\n","      <td>-3.807864</td>\n","      <td>0.615375</td>\n","      <td>1.249376</td>\n","      <td>...</td>\n","      <td>1.943465</td>\n","      <td>-1.015455</td>\n","      <td>0.057504</td>\n","      <td>-0.649709</td>\n","      <td>-0.415267</td>\n","      <td>-0.051634</td>\n","      <td>-1.206921</td>\n","      <td>-1.085339</td>\n","      <td>0.262698</td>\n","      <td>-0.994901</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.894286</td>\n","      <td>0.286157</td>\n","      <td>-0.113192</td>\n","      <td>-0.271526</td>\n","      <td>2.669599</td>\n","      <td>3.721818</td>\n","      <td>0.370145</td>\n","      <td>0.851084</td>\n","      <td>-0.392048</td>\n","      <td>-0.410430</td>\n","      <td>...</td>\n","      <td>-0.073425</td>\n","      <td>-0.268092</td>\n","      <td>-0.204233</td>\n","      <td>1.011592</td>\n","      <td>0.373205</td>\n","      <td>-0.384157</td>\n","      <td>0.011747</td>\n","      <td>0.142404</td>\n","      <td>0.994900</td>\n","      <td>-0.994901</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 30 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45a9a5af-9d28-4755-8143-aea22006849d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-45a9a5af-9d28-4755-8143-aea22006849d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-45a9a5af-9d28-4755-8143-aea22006849d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"NGF8DuqWJB_n"},"source":["# Train/Validation Feature 분포 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQE9etjZJB_n"},"outputs":[],"source":["# train_df.drop(columns=['ID']).hist(bins = 50, figsize = (20,20))\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mck7CfFqJB_o"},"outputs":[],"source":["# val_df.drop(columns=['ID', 'Class']).hist(bins = 50, figsize = (20,20))\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Qefu-12zJB_p"},"source":["# AutoEncoder를 이용한 학습"]},{"cell_type":"markdown","metadata":{"id":"p2XL1uwHJB_q"},"source":["## 하이퍼파라미터"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqfwUuiqJB_q"},"outputs":[],"source":["EPOCHS = 300 # 200개가,,,,적당해보임,,,,\n","LR = 1e-2\n","BS = 22768  # train 기준 5개로 나누기\n","SEED = 1234"]},{"cell_type":"markdown","metadata":{"id":"0OMY1hfTJB_r"},"source":["## 시드 고정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Qq2ZwjKJB_s"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED) # Seed 고정"]},{"cell_type":"markdown","metadata":{"id":"YEjilM6qJB_s"},"source":["## 데이터셋 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpAUvMLlJB_t"},"outputs":[],"source":["# 데이터 로더 파라미터\n","# https://subinium.github.io/pytorch-dataloader/\n","\n","class MyDataset(Dataset):\n","    def __init__(self, df, eval_mode):\n","        self.df = df\n","        self.eval_mode = eval_mode\n","        if self.eval_mode:\n","            self.labels = self.df['Class'].values\n","            self.df = self.df.drop(columns=['Class']).values\n","        else:\n","            self.df = self.df.values\n","        \n","    def __getitem__(self, index):\n","        if self.eval_mode:\n","            self.x = self.df[index]\n","            self.y = self.labels[index]\n","            # 3차원으로 만들어주는 것?\n","            return torch.Tensor(self.x), self.y\n","        else:\n","            self.x = self.df[index]\n","            return torch.Tensor(self.x)\n","        \n","    def __len__(self):\n","        return len(self.df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlWee0oHJB_u"},"outputs":[],"source":["train_dataset = MyDataset(df=train_df, eval_mode=False)\n","train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n","\n","# eval_mode = class 제거하기 위한 수단\n","# num_workers : CPU 코어 개수 설정\n","# https://jybaek.tistory.com/799\n","val_dataset = MyDataset(df = val_df, eval_mode=True)\n","val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"VKyx7OMYJB_v"},"source":["## 1D AutoEncoder\n","- 바꿀 수 있는 것\n","    - Linear 모델\n","    - Activation 함수\n","    - feature 숫자\n","    - 층을 더 쌓을 것인가?\n","    - backward 이용하여 진행"]},{"cell_type":"code","source":["# 가장 좋은 score - 90 / sample_submission_elu_1\n","\n","# batchNorm1d 사용 이유\n","# https://gaussian37.github.io/dl-concept-batchnorm/\n","\n","# 다양한 activation Function\n","# https://mlfromscratch.com/activation-functions-explained/#elu\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self):\n","        super(AutoEncoder, self).__init__()\n","        self.Encoder = nn.Sequential(\n","            nn.Linear(30,512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(),\n","            nn.Linear(512,128),\n","            nn.BatchNorm1d(128),\n","            nn.ELU(),\n","        )\n","        self.Decoder = nn.Sequential(\n","            nn.Linear(128,512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(),\n","            nn.Linear(512,30),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.Encoder(x)\n","        x = self.Decoder(x)\n","        return x"],"metadata":{"id":"uCP_XXDJS8uw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-uCjea3JB_w"},"outputs":[],"source":["# # 가장 좋은 score - 90 / sample_submission_elu_1\n","\n","# # batchNorm1d 사용 이유\n","# # https://gaussian37.github.io/dl-concept-batchnorm/\n","\n","# # 다양한 activation Function\n","# # https://mlfromscratch.com/activation-functions-explained/#elu\n","\n","# class AutoEncoder(nn.Module):\n","#     def __init__(self):\n","#         super(AutoEncoder, self).__init__()\n","#         self.Encoder = nn.Sequential(\n","#             nn.Linear(30,128),\n","#             nn.BatchNorm1d(128),\n","#             nn.ELU(),\n","#             nn.Linear(128,512),\n","#             nn.BatchNorm1d(512),\n","#             nn.ELU(),\n","#         )\n","#         self.Decoder = nn.Sequential(\n","#             nn.Linear(512,128),\n","#             nn.BatchNorm1d(128),\n","#             nn.ELU(),\n","#             nn.Linear(128,30),\n","#         )\n","        \n","#     def forward(self, x):\n","#         x = self.Encoder(x)\n","#         x = self.Decoder(x)\n","#         return x"]},{"cell_type":"markdown","metadata":{"id":"h0HFTXsoJB_w"},"source":["## Train 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-HMM0_uJB_x"},"outputs":[],"source":["class Trainer():\n","    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.scheduler = scheduler\n","        self.device = device\n","        # Loss Function\n","        # 파이토치 자주쓰는 Loss Function 정리 : https://nuguziii.github.io/dev/dev-002/\n","        self.criterion = nn.L1Loss().to(self.device)\n","        \n","    def fit(self, ):\n","        self.model.to(self.device)\n","        best_score = 0\n","        for epoch in range(EPOCHS):\n","            self.model.train()\n","            train_loss = []\n","            for x in iter(self.train_loader):\n","                x = x.float().to(self.device)\n","                # gradients 값들을 추후에 backward 해줄때 계속 더해주기 때문에 zero_grad() 사용\n","                # https://algopoolja.tistory.com/55\n","                self.optimizer.zero_grad()\n","\n","                _x = self.model(x)\n","                loss = self.criterion(x, _x)\n","                # 예측 손실 역전파\n","                loss.backward()\n","                # 역전파 단계에서 수집된 변화도로 매개변수 조정\n","                # https://tutorials.pytorch.kr/beginner/basics/optimization_tutorial.html\n","                self.optimizer.step()\n","\n","                train_loss.append(loss.item())\n","\n","            score = self.validation(self.model, 0.95)\n","            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n","\n","            if self.scheduler is not None:\n","                self.scheduler.step(score)\n","\n","            if best_score < score:\n","                best_score = score\n","                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n","    \n","    def validation(self, eval_model, thr):\n","        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        eval_model.eval()\n","        pred = []\n","        true = []\n","        with torch.no_grad():\n","            for x, y in iter(self.val_loader):\n","                x = x.float().to(self.device)\n","\n","                _x = self.model(x)\n","                diff = cos(x, _x).cpu().tolist()\n","                # diff(코사인유사도)가 thr(임계값) 보다 작으면 1, 아님 0\n","                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n","                pred += batch_pred\n","                true += y.tolist()\n","\n","        return f1_score(true, pred, average='macro')"]},{"cell_type":"markdown","metadata":{"id":"JYaKm5D0JB_y"},"source":["## 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cp1-xvrrJB_y","outputId":"2647598a-2343-4814-a246-f32adfd3c7fd","executionInfo":{"status":"ok","timestamp":1658752232361,"user_tz":-540,"elapsed":489069,"user":{"displayName":"최수빈","userId":"13285258253583110857"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : [0] Train loss : [0.6644479582707087] Val Score : [0.0017566050423447731])\n","Epoch : [1] Train loss : [0.4717100312312444] Val Score : [0.09552672267053802])\n","Epoch : [2] Train loss : [0.4281467745701472] Val Score : [0.1837759815808481])\n","Epoch : [3] Train loss : [0.38747307658195496] Val Score : [0.24287353309957038])\n","Epoch : [4] Train loss : [0.39491219570239383] Val Score : [0.2499195418711346])\n","Epoch : [5] Train loss : [0.33341246594985324] Val Score : [0.2505779990158661])\n","Epoch : [6] Train loss : [0.4239070266485214] Val Score : [0.2504983635019693])\n","Epoch : [7] Train loss : [0.4385666201512019] Val Score : [0.2117439999762083])\n","Epoch : [8] Train loss : [0.3876720741391182] Val Score : [0.34866689250145766])\n","Epoch : [9] Train loss : [0.435342937707901] Val Score : [0.2165445705820817])\n","Epoch : [10] Train loss : [0.4526946345965068] Val Score : [0.2980052683926517])\n","Epoch : [11] Train loss : [0.4616596922278404] Val Score : [0.10357174913551635])\n","Epoch : [12] Train loss : [0.40515433500210446] Val Score : [0.34022349435989235])\n","Epoch : [13] Train loss : [0.30478254954020184] Val Score : [0.3309476144724741])\n","Epoch : [14] Train loss : [0.3420894344647725] Val Score : [0.3718305725743782])\n","Epoch : [15] Train loss : [0.354611578087012] Val Score : [0.34696835794137626])\n","Epoch : [16] Train loss : [0.35864662130673725] Val Score : [0.36598433007045017])\n","Epoch : [17] Train loss : [0.34134819606939953] Val Score : [0.42612391464150867])\n","Epoch : [18] Train loss : [0.3464279721180598] Val Score : [0.3991584253476764])\n","Epoch : [19] Train loss : [0.2908308357000351] Val Score : [0.3493414480810287])\n","Epoch : [20] Train loss : [0.421923004090786] Val Score : [0.3717739264551968])\n","Epoch : [21] Train loss : [0.39089074979225796] Val Score : [0.37275808815136735])\n","Epoch : [22] Train loss : [0.34594083329041797] Val Score : [0.435593248551733])\n","Epoch : [23] Train loss : [0.31145546088616055] Val Score : [0.4030372429250507])\n","Epoch : [24] Train loss : [0.37915680805842084] Val Score : [0.4662006533163871])\n","Epoch : [25] Train loss : [0.3525840515891711] Val Score : [0.43595176557938664])\n","Epoch : [26] Train loss : [0.27778082092603046] Val Score : [0.44222908082429496])\n","Epoch : [27] Train loss : [0.34040551632642746] Val Score : [0.432210026945446])\n","Epoch : [28] Train loss : [0.30118727684020996] Val Score : [0.4245303764910116])\n","Epoch : [29] Train loss : [0.3336164802312851] Val Score : [0.4726220064935333])\n","Epoch : [30] Train loss : [0.34625503420829773] Val Score : [0.46012653046762464])\n","Epoch : [31] Train loss : [0.3222643906871478] Val Score : [0.4336984885939844])\n","Epoch : [32] Train loss : [0.369782418012619] Val Score : [0.4966929239021871])\n","Epoch : [33] Train loss : [0.277551864584287] Val Score : [0.4099112127981128])\n","Epoch : [34] Train loss : [0.43767866988976795] Val Score : [0.39235873363766055])\n","Epoch : [35] Train loss : [0.31404297053813934] Val Score : [0.4246887975170516])\n","Epoch : [36] Train loss : [0.31127607574065524] Val Score : [0.4598946180637717])\n","Epoch : [37] Train loss : [0.3093021288514137] Val Score : [0.4996949036972044])\n","Epoch : [38] Train loss : [0.3238465338945389] Val Score : [0.48929919535081784])\n","Epoch : [39] Train loss : [0.3295472239454587] Val Score : [0.4767370553297157])\n","Epoch : [40] Train loss : [0.30030667036771774] Val Score : [0.498341502639816])\n","Epoch : [41] Train loss : [0.2750069200992584] Val Score : [0.4775648991463187])\n","Epoch : [42] Train loss : [0.35403361916542053] Val Score : [0.5198351612363772])\n","Epoch : [43] Train loss : [0.29392606765031815] Val Score : [0.49474819750928556])\n","Epoch : [44] Train loss : [0.31089381128549576] Val Score : [0.5009380973388867])\n","Epoch : [45] Train loss : [0.303821158905824] Val Score : [0.5094036395488671])\n","Epoch : [46] Train loss : [0.35174038261175156] Val Score : [0.47414709099781926])\n","Epoch : [47] Train loss : [0.3237900361418724] Val Score : [0.4735500282503075])\n","Epoch : [48] Train loss : [0.2659479007124901] Val Score : [0.5275341662507901])\n","Epoch : [49] Train loss : [0.3170529107252757] Val Score : [0.5173745686891588])\n","Epoch : [50] Train loss : [0.2716307242711385] Val Score : [0.5164042751308315])\n","Epoch : [51] Train loss : [0.28821595509847003] Val Score : [0.5499639140767333])\n","Epoch : [52] Train loss : [0.2983989715576172] Val Score : [0.4955875977794851])\n","Epoch : [53] Train loss : [0.2916441485285759] Val Score : [0.4821291476938771])\n","Epoch : [54] Train loss : [0.2871517514189084] Val Score : [0.46448099393602416])\n","Epoch : [55] Train loss : [0.33266275624434155] Val Score : [0.46787951338662503])\n","Epoch : [56] Train loss : [0.26572345445553464] Val Score : [0.49073238319310725])\n","Epoch : [57] Train loss : [0.2627628768483798] Val Score : [0.516540447465794])\n","Epoch : [58] Train loss : [0.30839984863996506] Val Score : [0.49667480715620926])\n","Epoch : [59] Train loss : [0.2649443969130516] Val Score : [0.515440006945843])\n","Epoch : [60] Train loss : [0.2737824966510137] Val Score : [0.5062105708069892])\n","Epoch : [61] Train loss : [0.3204794302582741] Val Score : [0.48561368046391257])\n","Epoch : [62] Train loss : [0.26963453243176144] Val Score : [0.49119457680112966])\n","Epoch 00063: reducing learning rate of group 0 to 5.0000e-03.\n","Epoch : [63] Train loss : [0.23989168306191763] Val Score : [0.5783489094798163])\n","Epoch : [64] Train loss : [0.21554498250285783] Val Score : [0.6205848343502564])\n","Epoch : [65] Train loss : [0.2070466217895349] Val Score : [0.7187349645015549])\n","Epoch : [66] Train loss : [0.2496645711362362] Val Score : [0.6803845814211198])\n","Epoch : [67] Train loss : [0.2147868201136589] Val Score : [0.6727833548589096])\n","Epoch : [68] Train loss : [0.24057974169651666] Val Score : [0.7376112450647377])\n","Epoch : [69] Train loss : [0.2390155829489231] Val Score : [0.5288765154286703])\n","Epoch : [70] Train loss : [0.18885708351929983] Val Score : [0.5499639140767333])\n","Epoch : [71] Train loss : [0.21151484673221907] Val Score : [0.75467969893057])\n","Epoch : [72] Train loss : [0.22209824124972025] Val Score : [0.6692138080728531])\n","Epoch : [73] Train loss : [0.23795533056060472] Val Score : [0.5083083165114907])\n","Epoch : [74] Train loss : [0.1808863083521525] Val Score : [0.5535042161494685])\n","Epoch : [75] Train loss : [0.21870399390657744] Val Score : [0.6946258319247834])\n","Epoch : [76] Train loss : [0.18742510179678598] Val Score : [0.7226686263465465])\n","Epoch : [77] Train loss : [0.2679649281005065] Val Score : [0.4658167576351927])\n","Epoch : [78] Train loss : [0.19078761835892996] Val Score : [0.47396778461962863])\n","Epoch : [79] Train loss : [0.22174113864699999] Val Score : [0.697743660770528])\n","Epoch : [80] Train loss : [0.1834761512776216] Val Score : [0.6303451193287974])\n","Epoch : [81] Train loss : [0.20820654556155205] Val Score : [0.7655703273293624])\n","Epoch : [82] Train loss : [0.19848808397849402] Val Score : [0.7655703273293624])\n","Epoch : [83] Train loss : [0.20629590998093286] Val Score : [0.752094104263044])\n","Epoch : [84] Train loss : [0.19535248850782713] Val Score : [0.6858275973539593])\n","Epoch : [85] Train loss : [0.19168483838438988] Val Score : [0.7130854976190938])\n","Epoch : [86] Train loss : [0.1938189243276914] Val Score : [0.777425875747303])\n","Epoch : [87] Train loss : [0.23126746714115143] Val Score : [0.6029354027598012])\n","Epoch : [88] Train loss : [0.18727799008289972] Val Score : [0.6354400298239444])\n","Epoch : [89] Train loss : [0.20178096989790598] Val Score : [0.7805557779616334])\n","Epoch : [90] Train loss : [0.2193287213643392] Val Score : [0.6901265786034538])\n","Epoch : [91] Train loss : [0.17631349712610245] Val Score : [0.7743645687973808])\n","Epoch : [92] Train loss : [0.19517585883537927] Val Score : [0.7187349645015549])\n","Epoch : [93] Train loss : [0.18624707932273546] Val Score : [0.7743645687973808])\n","Epoch : [94] Train loss : [0.1878554088373979] Val Score : [0.7600119366040216])\n","Epoch : [95] Train loss : [0.18719823410113653] Val Score : [0.752094104263044])\n","Epoch : [96] Train loss : [0.20433180530865988] Val Score : [0.7903809848799157])\n","Epoch : [97] Train loss : [0.1804280330737432] Val Score : [0.808369294415656])\n","Epoch : [98] Train loss : [0.19763458147644997] Val Score : [0.8009145358549308])\n","Epoch : [99] Train loss : [0.20285625010728836] Val Score : [0.762761970120889])\n","Epoch : [100] Train loss : [0.20420977224906287] Val Score : [0.79380975986869])\n","Epoch : [101] Train loss : [0.1894840324918429] Val Score : [0.7573184229436457])\n","Epoch : [102] Train loss : [0.1956717682381471] Val Score : [0.808369294415656])\n","Epoch : [103] Train loss : [0.26900510862469673] Val Score : [0.6369672195553023])\n","Epoch : [104] Train loss : [0.23338733737667403] Val Score : [0.5252019027058485])\n","Epoch : [105] Train loss : [0.21397794410586357] Val Score : [0.8122361199071142])\n","Epoch : [106] Train loss : [0.20823468764623007] Val Score : [0.8202665410912253])\n","Epoch : [107] Train loss : [0.21081950763861337] Val Score : [0.8122361199071142])\n","Epoch : [108] Train loss : [0.19622850169738135] Val Score : [0.79380975986869])\n","Epoch : [109] Train loss : [0.1775943028430144] Val Score : [0.8009145358549308])\n","Epoch : [110] Train loss : [0.16213087737560272] Val Score : [0.8009145358549308])\n","Epoch : [111] Train loss : [0.20877287040154138] Val Score : [0.7399094305905288])\n","Epoch : [112] Train loss : [0.1961223470667998] Val Score : [0.8009145358549308])\n","Epoch : [113] Train loss : [0.20488840093215308] Val Score : [0.7837566139258728])\n","Epoch : [114] Train loss : [0.2047955480714639] Val Score : [0.8287186884323108])\n","Epoch : [115] Train loss : [0.3060699502627055] Val Score : [0.4978792516122071])\n","Epoch : [116] Train loss : [0.185629952698946] Val Score : [0.5882490710250109])\n","Epoch : [117] Train loss : [0.17719599977135658] Val Score : [0.7187349645015549])\n","Epoch : [118] Train loss : [0.20524373402198157] Val Score : [0.8287186884323108])\n","Epoch : [119] Train loss : [0.1928675721089045] Val Score : [0.8202665410912253])\n","Epoch : [120] Train loss : [0.17918420831362405] Val Score : [0.8287186884323108])\n","Epoch : [121] Train loss : [0.20225500812133154] Val Score : [0.7973199624677456])\n","Epoch : [122] Train loss : [0.24299825355410576] Val Score : [0.8162006166001039])\n","Epoch : [123] Train loss : [0.1936411571999391] Val Score : [0.8045965667777433])\n","Epoch : [124] Train loss : [0.23309648782014847] Val Score : [0.5275341662507901])\n","Epoch : [125] Train loss : [0.19864922886093458] Val Score : [0.5663027380505156])\n","Epoch 00126: reducing learning rate of group 0 to 2.5000e-03.\n","Epoch : [126] Train loss : [0.17842392002542815] Val Score : [0.8202665410912253])\n","Epoch : [127] Train loss : [0.16483116025726] Val Score : [0.8162006166001039])\n","Epoch : [128] Train loss : [0.2156271549562613] Val Score : [0.8244378451249526])\n","Epoch : [129] Train loss : [0.17561592410008112] Val Score : [0.8287186884323108])\n","Epoch : [130] Train loss : [0.190057793011268] Val Score : [0.6011781478947628])\n","Epoch : [131] Train loss : [0.1814806399246057] Val Score : [0.8162006166001039])\n","Epoch : [132] Train loss : [0.18918844064076742] Val Score : [0.833113452596645])\n","Epoch : [133] Train loss : [0.19252566248178482] Val Score : [0.833113452596645])\n","Epoch : [134] Train loss : [0.1676024521390597] Val Score : [0.8162006166001039])\n","Epoch : [135] Train loss : [0.1723408872882525] Val Score : [0.8122361199071142])\n","Epoch : [136] Train loss : [0.13848893158137798] Val Score : [0.8162006166001039])\n","Epoch : [137] Train loss : [0.16123541568716368] Val Score : [0.8244378451249526])\n","Epoch : [138] Train loss : [0.15059431518117586] Val Score : [0.8162006166001039])\n","Epoch : [139] Train loss : [0.16477415834863982] Val Score : [0.8244378451249526])\n","Epoch : [140] Train loss : [0.17115043538312116] Val Score : [0.8287186884323108])\n","Epoch : [141] Train loss : [0.1539920742313067] Val Score : [0.8244378451249526])\n","Epoch : [142] Train loss : [0.15347403412063917] Val Score : [0.8376267560436427])\n","Epoch : [143] Train loss : [0.1572858647753795] Val Score : [0.8376267560436427])\n","Epoch : [144] Train loss : [0.14492833303908506] Val Score : [0.8470287373843977])\n","Epoch : [145] Train loss : [0.22430225585897764] Val Score : [0.762761970120889])\n","Epoch : [146] Train loss : [0.1495832446962595] Val Score : [0.8376267560436427])\n","Epoch : [147] Train loss : [0.15521309711039066] Val Score : [0.8287186884323108])\n","Epoch : [148] Train loss : [0.15471376230319342] Val Score : [0.7903809848799157])\n","Epoch : [149] Train loss : [0.21713175189991793] Val Score : [0.7805557779616334])\n","Epoch : [150] Train loss : [0.15441233354310194] Val Score : [0.8244378451249526])\n","Epoch : [151] Train loss : [0.16324536129832268] Val Score : [0.8287186884323108])\n","Epoch : [152] Train loss : [0.15948773423830667] Val Score : [0.8376267560436427])\n","Epoch : [153] Train loss : [0.1553863361477852] Val Score : [0.833113452596645])\n","Epoch : [154] Train loss : [0.13239698484539986] Val Score : [0.8244378451249526])\n","Epoch : [155] Train loss : [0.16236827398339906] Val Score : [0.8376267560436427])\n","Epoch 00156: reducing learning rate of group 0 to 1.2500e-03.\n","Epoch : [156] Train loss : [0.16621927668650946] Val Score : [0.8376267560436427])\n","Epoch : [157] Train loss : [0.14234890043735504] Val Score : [0.8202665410912253])\n","Epoch : [158] Train loss : [0.1423710466672977] Val Score : [0.808369294415656])\n","Epoch : [159] Train loss : [0.1836227234452963] Val Score : [0.8162006166001039])\n","Epoch : [160] Train loss : [0.1819284732143084] Val Score : [0.8287186884323108])\n","Epoch : [161] Train loss : [0.17855552894373736] Val Score : [0.8122361199071142])\n","Epoch : [162] Train loss : [0.14269652776420116] Val Score : [0.8122361199071142])\n","Epoch : [163] Train loss : [0.1701253540813923] Val Score : [0.8162006166001039])\n","Epoch : [164] Train loss : [0.18071244346598783] Val Score : [0.833113452596645])\n","Epoch : [165] Train loss : [0.13976148950556913] Val Score : [0.8422634702634115])\n","Epoch : [166] Train loss : [0.18915075125793615] Val Score : [0.8376267560436427])\n","Epoch 00167: reducing learning rate of group 0 to 6.2500e-04.\n","Epoch : [167] Train loss : [0.18122446723282337] Val Score : [0.714936337281296])\n","Epoch : [168] Train loss : [0.129799614350001] Val Score : [0.833113452596645])\n","Epoch : [169] Train loss : [0.15571215314169726] Val Score : [0.8376267560436427])\n","Epoch : [170] Train loss : [0.13541333749890327] Val Score : [0.8470287373843977])\n","Epoch : [171] Train loss : [0.1555647092560927] Val Score : [0.8470287373843977])\n","Epoch : [172] Train loss : [0.15495385664204755] Val Score : [0.8470287373843977])\n","Epoch : [173] Train loss : [0.18006817686061063] Val Score : [0.8376267560436427])\n","Epoch : [174] Train loss : [0.16090253678460917] Val Score : [0.833113452596645])\n","Epoch : [175] Train loss : [0.15311152612169585] Val Score : [0.8376267560436427])\n","Epoch : [176] Train loss : [0.19140919856727123] Val Score : [0.8470287373843977])\n","Epoch : [177] Train loss : [0.13825015909969807] Val Score : [0.8376267560436427])\n","Epoch 00178: reducing learning rate of group 0 to 3.1250e-04.\n","Epoch : [178] Train loss : [0.1426007238527139] Val Score : [0.8422634702634115])\n","Epoch : [179] Train loss : [0.13466807082295418] Val Score : [0.833113452596645])\n","Epoch : [180] Train loss : [0.16502609166006246] Val Score : [0.8422634702634115])\n","Epoch : [181] Train loss : [0.13339824912448725] Val Score : [0.8422634702634115])\n","Epoch : [182] Train loss : [0.1738378033041954] Val Score : [0.8470287373843977])\n","Epoch : [183] Train loss : [0.13219143698612848] Val Score : [0.8470287373843977])\n","Epoch : [184] Train loss : [0.13837100813786188] Val Score : [0.8470287373843977])\n","Epoch : [185] Train loss : [0.16941675481696924] Val Score : [0.8422634702634115])\n","Epoch : [186] Train loss : [0.13811926543712616] Val Score : [0.8519279892324237])\n","Epoch : [187] Train loss : [0.16798237462838492] Val Score : [0.8376267560436427])\n","Epoch : [188] Train loss : [0.18817994557321072] Val Score : [0.8376267560436427])\n","Epoch : [189] Train loss : [0.15900759398937225] Val Score : [0.8422634702634115])\n","Epoch : [190] Train loss : [0.18364090410371622] Val Score : [0.8376267560436427])\n","Epoch : [191] Train loss : [0.1910374735792478] Val Score : [0.8287186884323108])\n","Epoch : [192] Train loss : [0.20922202741106352] Val Score : [0.6901265786034538])\n","Epoch : [193] Train loss : [0.15480483633776507] Val Score : [0.8519279892324237])\n","Epoch : [194] Train loss : [0.16304895654320717] Val Score : [0.8470287373843977])\n","Epoch : [195] Train loss : [0.14391917176544666] Val Score : [0.8470287373843977])\n","Epoch : [196] Train loss : [0.14189996756613255] Val Score : [0.8470287373843977])\n","Epoch : [197] Train loss : [0.29103326983749866] Val Score : [0.5474631286456717])\n","Epoch 00198: reducing learning rate of group 0 to 1.5625e-04.\n","Epoch : [198] Train loss : [0.13271496320764223] Val Score : [0.6495805272607397])\n","Epoch : [199] Train loss : [0.15238969400525093] Val Score : [0.8422634702634115])\n","Epoch : [200] Train loss : [0.14841476827859879] Val Score : [0.8519279892324237])\n","Epoch : [201] Train loss : [0.18017278984189034] Val Score : [0.8519279892324237])\n","Epoch : [202] Train loss : [0.14726579251388708] Val Score : [0.8519279892324237])\n","Epoch : [203] Train loss : [0.16924831892053285] Val Score : [0.8519279892324237])\n","Epoch : [204] Train loss : [0.16134274378418922] Val Score : [0.8162006166001039])\n","Epoch : [205] Train loss : [0.14732742061217627] Val Score : [0.8519279892324237])\n","Epoch : [206] Train loss : [0.15986031107604504] Val Score : [0.8621517488551477])\n","Epoch : [207] Train loss : [0.164827611297369] Val Score : [0.8470287373843977])\n","Epoch : [208] Train loss : [0.1402586754411459] Val Score : [0.8519279892324237])\n","Epoch : [209] Train loss : [0.15219271865983805] Val Score : [0.8422634702634115])\n","Epoch : [210] Train loss : [0.1345515667150418] Val Score : [0.8470287373843977])\n","Epoch : [211] Train loss : [0.14282340928912163] Val Score : [0.856966968023358])\n","Epoch : [212] Train loss : [0.1773550615956386] Val Score : [0.8519279892324237])\n","Epoch : [213] Train loss : [0.13266213921209177] Val Score : [0.8470287373843977])\n","Epoch : [214] Train loss : [0.1785847650219997] Val Score : [0.856966968023358])\n","Epoch : [215] Train loss : [0.1529514795790116] Val Score : [0.856966968023358])\n","Epoch : [216] Train loss : [0.16264323145151138] Val Score : [0.856966968023358])\n","Epoch : [217] Train loss : [0.15468028684457144] Val Score : [0.8519279892324237])\n","Epoch 00218: reducing learning rate of group 0 to 7.8125e-05.\n","Epoch : [218] Train loss : [0.1302507600436608] Val Score : [0.856966968023358])\n","Epoch : [219] Train loss : [0.1446494081368049] Val Score : [0.8470287373843977])\n","Epoch : [220] Train loss : [0.1375463424871365] Val Score : [0.8422634702634115])\n","Epoch : [221] Train loss : [0.14602812876303992] Val Score : [0.8376267560436427])\n","Epoch : [222] Train loss : [0.1580796738465627] Val Score : [0.8422634702634115])\n","Epoch : [223] Train loss : [0.1748564951121807] Val Score : [0.8519279892324237])\n","Epoch : [224] Train loss : [0.14150184455017248] Val Score : [0.8422634702634115])\n","Epoch : [225] Train loss : [0.11570254837473233] Val Score : [0.8621517488551477])\n","Epoch : [226] Train loss : [0.16273966866234937] Val Score : [0.856966968023358])\n","Epoch : [227] Train loss : [0.15790318325161934] Val Score : [0.8470287373843977])\n","Epoch : [228] Train loss : [0.15403139404952526] Val Score : [0.8470287373843977])\n","Epoch 00229: reducing learning rate of group 0 to 3.9063e-05.\n","Epoch : [229] Train loss : [0.1261597548921903] Val Score : [0.856966968023358])\n","Epoch : [230] Train loss : [0.15924043270448843] Val Score : [0.8470287373843977])\n","Epoch : [231] Train loss : [0.19321945309638977] Val Score : [0.6236778083350962])\n","Epoch : [232] Train loss : [0.1514767905076345] Val Score : [0.8519279892324237])\n","Epoch : [233] Train loss : [0.14364143833518028] Val Score : [0.8519279892324237])\n","Epoch : [234] Train loss : [0.15752635523676872] Val Score : [0.8470287373843977])\n","Epoch : [235] Train loss : [0.16129480364422003] Val Score : [0.8470287373843977])\n","Epoch : [236] Train loss : [0.3445553543666999] Val Score : [0.4902240974338784])\n","Epoch : [237] Train loss : [0.15439243552585444] Val Score : [0.5655124185105593])\n","Epoch : [238] Train loss : [0.14462097734212875] Val Score : [0.8202665410912253])\n","Epoch : [239] Train loss : [0.13277092762291431] Val Score : [0.8470287373843977])\n","Epoch 00240: reducing learning rate of group 0 to 1.9531e-05.\n","Epoch : [240] Train loss : [0.1717096051822106] Val Score : [0.8470287373843977])\n","Epoch : [241] Train loss : [0.3277172750482957] Val Score : [0.45321435777510033])\n","Epoch : [242] Train loss : [0.1451709307730198] Val Score : [0.6468806283000899])\n","Epoch : [243] Train loss : [0.16270575175682703] Val Score : [0.8422634702634115])\n","Epoch : [244] Train loss : [0.1580240074545145] Val Score : [0.8422634702634115])\n","Epoch : [245] Train loss : [0.14352138278385004] Val Score : [0.8519279892324237])\n","Epoch : [246] Train loss : [0.1482100294282039] Val Score : [0.8470287373843977])\n","Epoch : [247] Train loss : [0.15381468025346598] Val Score : [0.856966968023358])\n","Epoch : [248] Train loss : [0.15390188495318094] Val Score : [0.8470287373843977])\n","Epoch : [249] Train loss : [0.13867852029701075] Val Score : [0.8470287373843977])\n","Epoch : [250] Train loss : [0.143180167923371] Val Score : [0.8422634702634115])\n","Epoch 00251: reducing learning rate of group 0 to 9.7656e-06.\n","Epoch : [251] Train loss : [0.13757069346805414] Val Score : [0.8422634702634115])\n","Epoch : [252] Train loss : [0.1610050443559885] Val Score : [0.856966968023358])\n","Epoch : [253] Train loss : [0.1602941919118166] Val Score : [0.8519279892324237])\n","Epoch : [254] Train loss : [0.15620580998559794] Val Score : [0.8376267560436427])\n","Epoch : [255] Train loss : [0.13798621793588003] Val Score : [0.8376267560436427])\n","Epoch : [256] Train loss : [0.16525943825642267] Val Score : [0.7805557779616334])\n","Epoch : [257] Train loss : [0.15842645553251108] Val Score : [0.8162006166001039])\n","Epoch : [258] Train loss : [0.14321730161706606] Val Score : [0.8376267560436427])\n","Epoch : [259] Train loss : [0.16118224275608858] Val Score : [0.8422634702634115])\n","Epoch : [260] Train loss : [0.14563367329537868] Val Score : [0.8470287373843977])\n","Epoch : [261] Train loss : [0.16558213718235493] Val Score : [0.8470287373843977])\n","Epoch 00262: reducing learning rate of group 0 to 4.8828e-06.\n","Epoch : [262] Train loss : [0.15730714735885462] Val Score : [0.8470287373843977])\n","Epoch : [263] Train loss : [0.1600135900080204] Val Score : [0.8422634702634115])\n","Epoch : [264] Train loss : [0.4687062570204337] Val Score : [0.5084788454280785])\n","Epoch : [265] Train loss : [0.1484589216609796] Val Score : [0.543110595636484])\n","Epoch : [266] Train loss : [0.13617079084118208] Val Score : [0.6011781478947628])\n","Epoch : [267] Train loss : [0.1553380104402701] Val Score : [0.833113452596645])\n","Epoch : [268] Train loss : [0.1525737556318442] Val Score : [0.856966968023358])\n","Epoch : [269] Train loss : [0.15403802258272967] Val Score : [0.856966968023358])\n","Epoch : [270] Train loss : [0.13794776735206446] Val Score : [0.8470287373843977])\n","Epoch : [271] Train loss : [0.1679847501218319] Val Score : [0.8470287373843977])\n","Epoch : [272] Train loss : [0.13582785551746687] Val Score : [0.8287186884323108])\n","Epoch 00273: reducing learning rate of group 0 to 2.4414e-06.\n","Epoch : [273] Train loss : [0.16074995510280132] Val Score : [0.8519279892324237])\n","Epoch : [274] Train loss : [0.13956421179076037] Val Score : [0.8422634702634115])\n","Epoch : [275] Train loss : [0.15474102459847927] Val Score : [0.8422634702634115])\n","Epoch : [276] Train loss : [0.13706526594857374] Val Score : [0.856966968023358])\n","Epoch : [277] Train loss : [0.13169105537235737] Val Score : [0.8519279892324237])\n","Epoch : [278] Train loss : [0.16651158531506857] Val Score : [0.856966968023358])\n","Epoch : [279] Train loss : [0.15228122721115747] Val Score : [0.8519279892324237])\n","Epoch : [280] Train loss : [0.13895037459830442] Val Score : [0.8470287373843977])\n","Epoch : [281] Train loss : [0.14827470605572066] Val Score : [0.8470287373843977])\n","Epoch : [282] Train loss : [0.13800062735875449] Val Score : [0.8470287373843977])\n","Epoch : [283] Train loss : [0.1559632650266091] Val Score : [0.8470287373843977])\n","Epoch 00284: reducing learning rate of group 0 to 1.2207e-06.\n","Epoch : [284] Train loss : [0.14809014089405537] Val Score : [0.8470287373843977])\n","Epoch : [285] Train loss : [0.15351580331722894] Val Score : [0.833113452596645])\n","Epoch : [286] Train loss : [0.1386879626661539] Val Score : [0.8422634702634115])\n","Epoch : [287] Train loss : [0.1616104214141766] Val Score : [0.8470287373843977])\n","Epoch : [288] Train loss : [0.17085786846776804] Val Score : [0.8470287373843977])\n","Epoch : [289] Train loss : [0.16919560295840105] Val Score : [0.8674887641844412])\n","Epoch : [290] Train loss : [0.1527079977095127] Val Score : [0.8470287373843977])\n","Epoch : [291] Train loss : [0.15656987763941288] Val Score : [0.8519279892324237])\n","Epoch : [292] Train loss : [0.1444968841969967] Val Score : [0.8470287373843977])\n","Epoch : [293] Train loss : [0.15603411880632242] Val Score : [0.8519279892324237])\n","Epoch : [294] Train loss : [0.14430151196817556] Val Score : [0.8519279892324237])\n","Epoch : [295] Train loss : [0.14192533368865648] Val Score : [0.856966968023358])\n","Epoch : [296] Train loss : [0.16558905752996603] Val Score : [0.8422634702634115])\n","Epoch : [297] Train loss : [0.14065583050251007] Val Score : [0.8519279892324237])\n","Epoch : [298] Train loss : [0.21845177250603834] Val Score : [0.6727833548589096])\n","Epoch : [299] Train loss : [0.15954933688044548] Val Score : [0.7805557779616334])\n"]}],"source":["model = nn.DataParallel(AutoEncoder())\n","model.eval()\n","# 옵티마이저 종류 : https://velog.io/@chang0517/Optimizer-%EC%A2%85%EB%A5%98-%EB%B0%8F-%EC%A0%95%EB%A6%AC\n","# pytorch optim 알고리즘 : https://pytorch.org/docs/stable/optim.html\n","optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n","\n","trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n","trainer.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzU9oCnqJB_y","executionInfo":{"status":"ok","timestamp":1658752232363,"user_tz":-540,"elapsed":63,"user":{"displayName":"최수빈","userId":"13285258253583110857"}},"outputId":"f7a832f5-0b92-42db-992f-549ae89c79c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataParallel(\n","  (module): AutoEncoder(\n","    (Encoder): Sequential(\n","      (0): Linear(in_features=30, out_features=512, bias=True)\n","      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ELU(alpha=1.0)\n","      (3): Linear(in_features=512, out_features=128, bias=True)\n","      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ELU(alpha=1.0)\n","    )\n","    (Decoder): Sequential(\n","      (0): Linear(in_features=128, out_features=512, bias=True)\n","      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ELU(alpha=1.0)\n","      (3): Linear(in_features=512, out_features=30, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":26}],"source":["model = AutoEncoder()\n","model.load_state_dict(torch.load('./best_model.pth'))\n","model = nn.DataParallel(model)\n","model.eval()"]},{"cell_type":"code","source":["test_df = pd.read_csv('/content/drive/MyDrive/[데이콘] 신용카드 거래/test.csv')\n","test_df = test_df.drop(columns=['ID'])"],"metadata":{"id":"P6zhKG0QMWN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = MyDataset(test_df, False)\n","test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=6)"],"metadata":{"id":"WHUHROh-Mg7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prediction(model, thr, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","    pred = []\n","    with torch.no_grad():\n","        for x in iter(test_loader):\n","            x = x.float().to(device)\n","            \n","            _x = model(x)\n","            \n","            diff = cos(x, _x).cpu().tolist()\n","            # diff(코사인유사도)가 thr(임계값) 보다 작으면 1, 아님 0\n","            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n","            pred += batch_pred\n","    return pred"],"metadata":{"id":"rsVlKE_KMjcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = prediction(model, 0.95, test_loader, device)"],"metadata":{"id":"_GBcNCGxMlDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit = pd.read_csv('/content/drive/MyDrive/[데이콘] 신용카드 거래/sample_submission.csv')\n","submit['Class'] = preds\n","submit.to_csv('/content/drive/MyDrive/[데이콘] 신용카드 거래/submit_autoencoder_elu_3.csv', index=False)"],"metadata":{"id":"T58OWUSzMozt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-nXzKoc_MwjW"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}